{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VellummyilumVinoth/Toxic_Comment_Classification/blob/main/LSTM_with_subword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Dropout, Embedding\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import fasttext.util\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load and preprocess the data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Dats/Kaggle/pre_data_train.csv')\n",
        "\n",
        "dataset, test = train_test_split(dataset, test_size=0.2)\n",
        "dataset.isnull().sum()\n",
        "\n",
        "sentiment = dataset['preprocessed_text'].values\n",
        "y_list = [\"toxic\",\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = dataset[y_list].values\n",
        "\n",
        "predict_data = test['preprocessed_text'].values\n",
        "\n",
        "# Convert float values to strings\n",
        "sentiment = np.array(sentiment, dtype=str)\n",
        "predict_data = np.array(predict_data, dtype=str)\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(list(sentiment))\n",
        "seq = tokenizer.texts_to_sequences(sentiment)\n",
        "pad = pad_sequences(seq, maxlen=100)\n",
        "test_seq = tokenizer.texts_to_sequences(predict_data)\n",
        "test_pad = pad_sequences(test_seq, maxlen=100)\n",
        "\n",
        "# Load FastText pre-trained subword embeddings\n",
        "fasttext.util.download_model('cc.en.300.bin')\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# Create subword embeddings for the vocabulary\n",
        "embedding_dim = 300  # Dimensionality of FastText subword embeddings\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = ft.get_word_vector(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the model architecture with subword embeddings\n",
        "def model_with_subword_embedding():\n",
        "    inputs = Input(shape=(100,))\n",
        "    x = Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=False)(inputs)\n",
        "    x = LSTM(50)(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(50, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_with_subword = model_with_subword_embedding()\n",
        "print(model_with_subword.summary())\n",
        "\n",
        "# Train the model with subword embeddings\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
        "model_with_subword.fit(pad, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=[early])\n",
        "\n",
        "model_with_subword.save('/content/drive/MyDrive/LSTM_with_subword')\n",
        "\n",
        "# Make predictions on the test data with subword embeddings\n",
        "y_test_with_subword = model_with_subword.predict([test_pad], batch_size=1024, verbose=1)\n",
        "\n",
        "# Apply threshold and convert to 0 or 1\n",
        "threshold = 0.16  # Adjust this threshold as needed\n",
        "y_test_with_subword[y_test_with_subword >= threshold] = 1\n",
        "y_test_with_subword[y_test_with_subword < threshold] = 0\n",
        "y_test_with_subword = y_test_with_subword.astype(int)\n",
        "\n",
        "# Create a new DataFrame with the predicted labels with subword embeddings\n",
        "predict_labels_df_with_subword = pd.DataFrame(y_test_with_subword, columns=y_list)\n",
        "\n",
        "# Create a new DataFrame with the predicted labels with subword embeddings\n",
        "predict_data_df = pd.DataFrame(predict_data, columns=[\"Title\"])\n",
        "\n",
        "# Combine the original DataFrame with the predicted labels with subword embeddings\n",
        "predict_df_with_subword = pd.concat([predict_data_df, predict_labels_df_with_subword], axis=1)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "predict_df_with_subword.to_csv('/content/drive/MyDrive/predict_lstm_with_subword.csv', index=False)\n",
        "\n",
        "# Assuming you have the true labels for the test data\n",
        "y_test_true = test[y_list].values\n",
        "\n",
        "# Apply threshold to convert to binary values\n",
        "y_test_true = np.where(y_test_true >= threshold, 1, 0)\n",
        "\n",
        "# Calculate accuracy and F1 score for the model with subword embeddings\n",
        "accuracy_with_subword = accuracy_score(y_test_true, y_test_with_subword)*100\n",
        "f1_score_with_subword = f1_score(y_test_true, y_test_with_subword, average='micro')*100\n",
        "\n",
        "# Print accuracy and F1 score\n",
        "print(\"Accuracy (with subword embeddings):\", accuracy_with_subword)\n",
        "print(\"F1 Score (with subword embeddings):\", f1_score_with_subword)\n"
      ],
      "metadata": {
        "id": "I5Ch0u81mpvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4FVXW2JrvoKs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFrv4raMxBST",
        "outputId": "25e705bd-2284-47c2-da12-7c732789ebfa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Dats/Kaggle/pre_data_train.csv')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset, test = train_test_split(dataset, test_size=0.2)\n",
        "\n",
        "sentiment = dataset['preprocessed_text'].values"
      ],
      "metadata": {
        "id": "0YEBiWOEvspR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_list = [\"toxic\",\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = dataset[y_list].values"
      ],
      "metadata": {
        "id": "WuJOZwI2vtb7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data = test['preprocessed_text'].values"
      ],
      "metadata": {
        "id": "-Oa6ro1qvwR4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert float values to strings\n",
        "sentiment = np.array(sentiment, dtype=str)\n",
        "predict_data = np.array(predict_data, dtype=str)\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(list(sentiment))\n",
        "seq = tokenizer.texts_to_sequences(sentiment)\n",
        "pad = pad_sequences(seq, maxlen=100)\n",
        "test_seq = tokenizer.texts_to_sequences(predict_data)\n",
        "test_pad = pad_sequences(test_seq, maxlen=100)\n"
      ],
      "metadata": {
        "id": "3NkJmMP0vxJ6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n"
      ],
      "metadata": {
        "id": "XzC4v9TDlBvV",
        "outputId": "5d46aef1-b1b4-4013-84c3-91fb887182d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393174 sha256=7d7f89b4a1026d6fff67c62d0b7d712924c901f7cd5611c04d92dc0d8437d829\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "\n",
        "# Download and load FastText subword embeddings\n",
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# Convert tokens to FastText subword embeddings\n",
        "subword_sequences = []\n",
        "\n",
        "for sentence in sentiment:\n",
        "    tokens = sentence.split()\n",
        "    subword_seq = []\n",
        "\n",
        "    for token in tokens:\n",
        "        subword_embed = ft.get_word_vector(token)\n",
        "        subword_seq.append(subword_embed)\n",
        "\n",
        "    subword_sequences.append(subword_seq)\n",
        "\n",
        "subword_pad = pad_sequences(subword_sequences, maxlen=100, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "cxC8wGtBkHTb",
        "outputId": "7f3f4f04-aa0a-4dea-dcc4-f6ce5bf2ea31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b0af37e7626c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Download and load FastText subword embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc.en.300.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/util/util.py\u001b[0m in \u001b[0;36mdownload_model\u001b[0;34m(lang_id, if_exists, dimension)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_download_gz_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgz_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgz_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/util/util.py\u001b[0m in \u001b[0;36m_download_gz_model\u001b[0;34m(gz_file_name, if_exists)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgz_file_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0m_download_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/util/util.py\u001b[0m in \u001b[0;36m_download_file\u001b[0;34m(url, write_file_name, chunk_size)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0m_print_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/util/util.py\u001b[0m in \u001b[0;36m_print_progress\u001b[0;34m(downloaded_bytes, total_size)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbar_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"]\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownloaded_bytes\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimport_lock_held\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "def model_with_subword_embedding():\n",
        "    inputs = Input(shape=(100, ))\n",
        "    x = Embedding(20000, 128)(inputs)\n",
        "    x = Bidirectional(LSTM(50))(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(50, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = model_with_subword_embedding()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "viZfpCjzv9Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with subword embeddings\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
        "model_with_subword.fit(subword_pad, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=[early])\n",
        "model_with_subword.save('/content/drive/MyDrive/LSTM_with_subword')\n"
      ],
      "metadata": {
        "id": "C2Oi-ZWfwFli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/LSTM_with_subword')"
      ],
      "metadata": {
        "id": "t32LZpSDemWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_test_with_subword = model_with_subword.predict([test_pad], batch_size=1024, verbose=1)\n",
        "\n",
        "# Apply threshold and convert to 0 or 1\n",
        "threshold = 0.16  # Adjust this threshold as needed\n",
        "y_test_with_subword[y_test_with_subword >= threshold] = 1\n",
        "y_test_with_subword[y_test_with_subword < threshold] = 0\n",
        "y_test_with_subword = y_test_with_subword.astype(int)\n",
        "\n",
        "# Create a new DataFrame with the predicted labels without subword embeddings\n",
        "predict_labels_df_with_subword = pd.DataFrame(y_test_with_subword, columns=y_list)\n",
        "\n",
        "# Create a new DataFrame with the predicted labels without subword embeddings\n",
        "predict_data_df = pd.DataFrame(predict_data, columns=[\"Title\"])\n",
        "\n",
        "# Combine the original DataFrame with the predicted labels without subword embeddings\n",
        "predict_df_with_subword = pd.concat([predict_data_df, predict_labels_df_with_subword], axis=1)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "predict_df_with_subword.to_csv('/content/drive/MyDrive/predict_lstm_with_subword.csv', index=False)"
      ],
      "metadata": {
        "id": "14of2SRkwH-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_df_with_subword"
      ],
      "metadata": {
        "id": "MoxfrMwM2Ruh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayp5vUOUvddS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Calculate accuracy and F1 score for the model with subword embeddings\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Assuming you have the true labels for the test data\n",
        "y_test_true = test[y_list].values\n",
        "\n",
        "# Apply threshold to convert to binary values\n",
        "y_test_true = np.where(y_test_true >= threshold, 1, 0)\n",
        "\n",
        "# Assuming you have the true labels for the test data in y_test_true\n",
        "accuracy_with_subword = accuracy_score(y_test_true, y_test_with_subword)*100\n",
        "f1_score_with_subword = f1_score(y_test_true, y_test_with_subword, average='micro')*100\n",
        "\n",
        "# Print accuracy and F1 score\n",
        "print(\"Accuracy (with subword embeddings):\", accuracy_with_subword)\n",
        "print(\"F1 Score (with subword embeddings):\", f1_score_with_subword)\n"
      ]
    }
  ]
}